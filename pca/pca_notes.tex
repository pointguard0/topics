\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{color}

\setlength{\oddsidemargin}{.25in}
\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6.25in}
\setlength{\topmargin}{-0.4in}
\setlength{\textheight}{8.5in}
\usepackage[russian,english]{babel}

\title{Notes on Principal Component Analysis (PCA)}
\author{Arshak Minasyan}
\date{January 2016}

\begin{document}

\maketitle

\section{Basics}

\subsection{Singular value decomposition (SVD)} 
\par SVD is defined for arbitrary matrix $X \in \mathbb{M}^{n \times d} (\mathbb{R})$ ($\mathbb{R}$ could be easily replaced by $\mathbb{C}$) and let $r = \text{rank } X$, then 
\begin{align}
\underbrace{X}_{n \times d} = \underbrace{U}_{n \times r} \underbrace{\Sigma}_{r \times r} \underbrace{V^T}_{r \times d},
\end{align}
where 
\begin{align}
U = \begin{pmatrix} \vdots &  & \vdots \\ u_1 & \dots & u_r \\ \vdots &  & \vdots \end{pmatrix} \quad \Sigma = \text{diag}(\sigma_1, \dots, \sigma_r) \quad V = \begin{pmatrix} \vdots &  & \vdots \\ v_1 & \dots & v_d \\ \vdots &  & \vdots \end{pmatrix} 
\end{align}
with $u_i \in \mathbb{R}^n$ for $i = 1, \dots, r$, $v_i \in \mathbb{R}^r$ for $i = 1, \dots, d$ and $\sigma_i > 0$ for $i = 1, \dots r$. Moreover, matrices $U$ and $V$ are unitary, i.e. $U^TU = I, V^TV = I$.
\subsection{Eckart-Young theorem}

\section{PCA}
Suppose we are given a cluster $\mathcal{L}_n = \{ x_1, \dots, x_n \}$, where $x_i \in \mathbb{R}^d$ for $i = 1, \dots, n$. We define matrix $X$ as follows 
\begin{align}
\underbrace{X}_{n \times d} = \begin{pmatrix} x_{11} & \dots & \dots & x_{1d} \\ \vdots  & \dots & \dots & \vdots \\ \vdots  & \dots & \dots & \vdots \\ x_{n1} & \dots & \dots & x_{nd} \end{pmatrix}
\end{align}
Then, we center our data by subtracting the sample mean from all observations, i.e. $x_i := x_i - \overline{x}$, where $\overline{x} = \dfrac 1 n \sum_{i=1}^n x_i \in \mathbb{R}^d$. So the sample covariance matrix could be written in this way
\begin{align}
S = \frac 1 n \sum_{i=1}^n x_i x_i^T = \frac 1 n X^T X
\end{align}
\par Thus, using singular value decomposition for matrix $X = U \Sigma V^T$ one can obtain
\begin{eqnarray}
X^TX = V\Sigma U^T U \Sigma V^T = V \Sigma^2 V^T \implies \underbrace{\frac 1 n X^TX}_{S} v_i = \frac{\sigma_i^2} n v_i,
\end{eqnarray}
which means that the pairs $\left(\frac {\sigma_i^2} {n}, v_i \right)$ are the eigenvalue-eigenvector pairs of matrix $S$. 
\par The $i$-th principal component of matrix $S$ is 
\begin{align}
\underbrace{z_i}_{n \times 1} = \underbrace{X}_{n \times d} \underbrace{v_i}_{d \times 1} \implies z_i = U \Sigma V^T v_i \implies z_i = \sigma_i u_i 
\end{align}
or, aggregating all the principal components one can get
\begin{align}
\underbrace{Z}_{n \times r} = \underbrace{X}_{n \times d} \underbrace{V}_{d \times r} = \underbrace{U}_{n \times r} \underbrace{\Sigma}_{r \times r},
\end{align}
where $Z = \begin{pmatrix} \vdots &  & \vdots \\ z_1 & \dots & z_r \\ \vdots &  & \vdots \end{pmatrix}
$.
\subsection{Kernel PCA}
Take a look at HSE french guy, at CS department. 
\subsection{Probabilistic PCA}
Vetrov SHAD.
\section{Robust PCA}
\end{document}
