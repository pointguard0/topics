\section{Bayesian Inference}\label{5}
We start this section with an interesting and simple exercise. 

\begin{exmp} 
Consider two doctors doing two type of surgeries, and the first doctor has better performance on both surgeries, separately. Is it possible that the second doctor has better performance overall? If yes, bring an example, otherwise prove that it is impossible.
\end{exmp}

\subsection{Maximum Likelihood}
Consider the maximum likelihood function: given an independent and identically distributed (i.i.d.) set of data from a density function $f_{\theta}$ with an unknown parameter $\theta \in \Theta$, the associated likelihood function is 
\begin{align}\label{likelihood}
    \mcL(\theta \, | \, x_1, \dots, x_n) = \prod_{i=1}^n f_{\theta}(x_i)
\end{align}
The standard parameter estimation technique is to maximize the (log) likelihood with respect to parameter $\theta \in \Theta$. Usually $\Theta = \RR$ which leads to , for more complicated cases one can use Lagrange  multipliers along with Karush-Kuhn-Tucker conditions to obtain 
\begin{align}
    \tilde{\theta} = \arg \max_{\theta \in \Theta} \mcL(\theta \, | \, x_1, \dots, x_n).    
\end{align}
Note that in some cases it might be convenient to maximize the logarithm of the likelihood function. The estimator obtained this way is known as maximum likelihood estimator (MLE).

\begin{exer} 
Estimate the probability of getting a tail $p$ of an unfair coin given $n$ experiments. What do you think is bad in this estimate?
\end{exer}
\subsection{Bayesian approach}
The major input of Bayesian approach, compared to the standard likelihood approach, is that it modifies the likelihood into a {\it{posterior}} distribution, taking into account what we believe about the parameter $\theta$ {\it{a priori}}. 
\par Applying Bayes rule and letting $\v{X} = \{ x_1, \dots, x_n \}$ we can obtain
\begin{align}\label{main}
\pi(\theta \, | \, \v{X}) = \frac {p(\v{X} \, | \, \theta) \pi_0(\theta)}{\int p(\v{X} \, | \, \theta) \pi_0(\theta) \, d\theta},
\end{align}
where $\pi_0(\theta)$ is prior distribution, $p(\v{X}, \theta)$ is the likelihood of $\v{X}$ given for $\theta$. $p(\v{X} \, | \, \theta)$ is a joint conditional probability of $\v{X}$ given $\theta$, and it can be decomposed as the product of one-dimensional densities (assuming iid $x_i$'s), i.e. 
\begin{align}
    p(\v{X} \, | \, \theta) = \prod_{i=1}^n p_{\theta}(x_i),
\end{align}
which is exactly what we have in \eqref{likelihood}.
\par Note that the expression \eqref{main} results in a distribution function of $\theta$ known as posterior distribution of $\theta$ given data $\v{X}$. 
\par In order to obtain a point estimate from the distribution function one can maximize the probability density function (pdf) of posterior distribution
\begin{align}
    \hat{\theta} = \arg \max_{\theta} \pi(\theta \, | \, \v{X}) = \arg\max_{\theta} \left[\,\, \overbrace{p(\v{X} \, | \, \theta)}^{\text{likelihood}} \underbrace{\pi_0(\theta)}_{\text{prior}} \,\, \right] \stackrel{\text{def}}{=} \hat{\theta}_{\text{MAP}},
\end{align}
which is called maximum a posteriori (MAP) estimator;
or take the expected value of it
\begin{align}
    \hat{\theta} = \mathbb{E} \pi(\theta \, | \, \v{X}) = \frac{\int \theta p(\v{X} \, | \, \theta) \pi_0(\theta) \, d\theta}{\int p(\v{X} \, | \, \theta) \pi_0(\theta) \, d\theta}
\end{align}
\begin{rmrk}
$\hat{\theta}_{\text{MAP}}$ converges to $\hat{\theta}_{\text{MLE}}$ when $n \to \infty$.
\end{rmrk}
\begin{exer} 
Estimate the probability of getting a tail $p$ of an unfair coin (maximizing the posterior probability) given $n$ experiments using a prior for $p$, $p \sim \mathcal{B}\textbf{eta}(\alpha, \beta)$ with density 
\begin{align}
p_{\alpha, \beta}(x) = \frac{x^{\alpha - 1} (1-x)^{\beta - 1}}{B(\alpha, \beta)} \quad B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)},
\end{align} 
which implies $p_{\alpha, \beta}(x) \propto x^{\alpha - 1} (1-x)^{\beta - 1}$. Comment on what you obtain.
\end{exer}

\subsection{Prior distributions}
In this section we introduce an important class of distributions, which is commonly used in Bayesian inference. 
