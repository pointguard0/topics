\section{Introduction and Literature Review}

\par Many statistical tasks can be viewed as problems of semiparametric estimation when the unknown data distribution is described by a high or infinite dimensional parameter while the target is of low dimension. Typical examples are provided by functional estimation, estimation of a function at a point, or simply by estimating a given subvector of the parameter vector. The classical statistical theory provides a general solution to this problem: estimate the full parameter vector by the maximum likelihood method and project the obtained estimate onto the target subspace. This approch is known as {\it{profile maximum likelihood}} and it appears to be {\it{semiparametrically efficient}} under some mild regularity conditions, which in case of Generalized Linear Models are satisfied. For more general case, for example, in M-estimation framework these technical conditions should be introduced separately and checked whether they are fulfilled or not.  We refer to the papers Murphy et al. (1999, 2000) and the book of Kosorok (2005) for a detailed presentation of the modern state of the theory and further references. The extension of these results can be found in Fan et al. (2001); Fan and Huang (2005); see also the references therein. 

This study revisits the problem of profile semiparametric estimation and addresses some new issues. One issue that is worth mentioning is the model misspecification. In most of the cases of practical problems, it is unrealistic to expect that the model assumptions are exactly fulfilled, even if some rich nonparametric models are used. This means that the true data distribution $\mathbb{P}$ does not belong to the considered parametric family, in our case --- exponential family. Applicability of the general semiparametric theory in such cases is questionable. An important feature of the presented approach is that it equally applies under a possible model misspecification. 

Let $\bm{\mathcal{Y}}$ denote the observed statistical model assumes that the unknown data distribution $\mathbb{P}$ belongs to a given parametric family ($\mathbb{P}_{\bm{v}}$):
\begin{eqnarray}
\bm{\mathcal{Y}} \sim \mathbb{P} = \mathbb{P}_{\bm{v}^*} \in (\mathbb{P}_{\bm{v}}, \bm{v} \in \Theta),
\end{eqnarray}
where $\Theta$ is some high dimensional or even infinite dimensional parametric space.
\par The maximum likelihood approach in the parametric estimation suggests to estimate the whole parameter vector $\bm{v}$ by maximizing the corresponding log-likelihood 
\begin{eqnarray*}
L(\bm{v}) = \log \frac {d\mathbb{P}_{\bm{v}}}{d \bm{\mu}_0} (\bm{\mathcal{Y}})
\end{eqnarray*}
for some dominating measure $\bm{\mu}_0$. Define the maximum likelihood estimator in the following way
\begin{eqnarray}
\tilde{\bm{v}} \stackrel{\mathrm{def}}{=} \arg\max_{\bm{v} \in \Theta} L(\bm{v}).
\end{eqnarray}
Our study admits a model specification $\mathbb{P} \notin (\mathbb{P}_{\bm{v}}, \bm{v} \in \Theta)$. Equivalently, one can say that $L(\bm{v})$ is the {\it{quasi log-likelihood function}} on $\Theta$. The {\it{target}} value $\bm{v}^*$ of the parameter $\bm{v}$ can be defined by 
\begin{eqnarray}
{\bm{v}}^* \stackrel{\mathrm{def}}{=} \arg\max_{\bm{v} \in \Theta} \mathbb{E} L(\bm{v}).
\end{eqnarray}
Under model misspecification, $\bm{v}^*$ defines the best parametric fit to $\mathbb{P}$ by the considered family.
\par The main point of the work is that the Alternating Method gives only a little gain, if any, in the complexity of optimum point computation for Linear Models, under some conditions on parameter dimensions, while in non-linear models the gain is pretty sensible. In non-linear models in most of cases the closed form solution can not be obtained, in some cases even the numerical solutions of first order conditions could be very hard to implement in full parameter dimension. The technique known as alternating maximization (minimization) helps in these situations and gives the estimation of parameter vector with adequate time complexity.  

The model that we consider has the parameter $\bm{v}$ which is of dimension $p+q$, where $p$ is the dimension of {\it{target}} parameter and $q$ is the dimension of {\it{nuisance}} parameter. Usually $p$ is not large, because we also care about tractability and interpretability of our model, but $q$ can be very large, although it is a {\it{nuisance}} parameter, we can not ignore and remove $\bm{\eta}$ from considered model.  Main problems with direct computations occur in high dimensions, i.e. in cases when $p+q$ is large enough to make it impossible to invert matrix of sizes $(p+q) \times (p+q)$, which is, in general, $\mathcal{O}((p+q)^3)$. Further we will only consider the case of finite $q$, the case of infinite $q$ is out of scope of this work.

The alternating maximization procedure can be understood as a special case of the Expectation Maximization algorithm (EM algorithm). The EM algorithm is a popular algorithm first derived by Dempster, Laird and Rubin  in 1977. Further on a number of modifications and extensions of this algorithm come into playground. Dempster et al. also described how EM algorithm can be implement in different fields and give fruitful results. We refer to the McLachlan and Krishman (1997) for the brief introduction to the development of EM algorithm. We restrict ourselves to citing the well-known convergence result by Wu in 1983, which is still state of the art in most settings. Unfortunately, Wu's result - as most convergence results on these interative procedures - only ensures convergence to some set of local maximizers or fixpoints of the procedure. In this work we consider one of the special cases where it is possible to show the actual convergence of the method. 

The work has the following structure. In Section 2 we introduce our reader with Alternating Method for Linear Models (LMs) and prove that the method converges to the likelihood estimator by exponentially fast convergence rate for any initial point $\bm{\theta}^{\circ}$. Section 3 contains the preliminaries about Generalized Linear Models (GLMs) and class of random variables called Exponential Family (EF). Fisher and Wilks theorems are provided in the GLM framework as well as local concentration condition. Section 4 extends the results from Section 2 using the tools explained in Section 3 and Taylor expansion of second order along with the Fisher expansion in the framework of GLMs.  Section 5 illustrates how the algorithm of alternating least squares (ALS) works and confirms above theoretical results using both real and simulated data. Section 6 contains concluding remarks. 

