\section{Concluding Remarks}
\par In this section we will sum up the results obtained above, discuss some practical aspects of this work and point where this method can be fruitful. We started this work with introduction to the semiparametric estimation and brief literature review of this field. Then we specify the problem which will be discussed in details.  
\par We start introducing the Alternating Least Squares (ALS) for the simplest case --- Linear Models (LMs). The complexity of ALS algorithm in the framework of LMs is asymptotically the same as the complexity of Maximum Likelihood Estimator (MLE), but still there is a gain. Theorem \eqref{linear} gives a condition for the convergence, moreover in linear case the convergence rate is exponential. Due to the easiness of the ALS algorithm implementation it could derive the optimum faster.
\par In Section 3 we provide a background to Generalized Linear Models (GLMs) and introduce  to important objects, such as, Fisher and covariance matrices and their block-representations, the class of Exponential Family (EF) of error distributions, concavity of log-likelihood function and the concentration property, which will be appealed further for obtaining main results and conditions.
\par Our main results are collected in Section 4 in Theorem \eqref{thm1} and \eqref{thm2}. First of all, we give the definition of alternating maximization in general and then come up with these two theorems. Theorem \eqref{thm1} gives the conditions for the linear convergence of alternating procedure applied to the Taylor expansion of $L(\bm{v})$. Using the same conditions as in Theorem \eqref{thm1} Theorem \eqref{thm2} established the distance between the estimator of {\it{target}} parameter obtained by ALS $\hat{\bm{\theta}}_k$ at step $k$ and the oracle choice $\bm{\theta}^* = \Pi_{\bm{\theta}} {\bm{v}}^* \stackrel{\mathrm{def}}{=} \Pi_{\bm{\theta}}\arg\max_{\bm{v}}\mathbb{E}L(\bm{v})$. These results are a bit technical but they contain all the conditions that should be checked before claiming the convergence of the method. The complexity of described method is $\mathcal{O}(N^3)$, where $N \stackrel{\mathrm{def}}{=} \max\{p, q\}$, while for the author's best knowledge the problem of  finding the optimum for any concave function in high dimensions can not be found in polynomial time.
\par Section 5 contains the illustration of the alternating maximization technique  procedure and a numerical example that confirms the convergence. Hence, while in linear case the gain is only in the constant in finite sample case and the asymptotic complexity remains the same, in GLMs framework we obtain again the same complexity as for LMs pointing out that the original problem is generally unsolved. 
\par Summing up, we see that described algorithm can be easily implemented in fields, where many factors needed to be included in the model. We have already discussed few possible applications of this algorithm in a number of different fields, such as, economics, medicine and bioinformatics, Information technology (IT) and it is just a little part of all possible fields. 
\par Moreover, we established a connection between Alternating Maximization (AM) and Expectation Maximization (EM) algorithms. The EM algorithm itself was derived by Dempster et. al (1977). Due to the fact that EM algorithm is widely used in Machine Learning our algorithm can be also applied to a large variety of Machine Learning problems.