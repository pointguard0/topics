\select@language {english}
\contentsline {part}{Part\ I\hskip \betweenumberspace Model selection for linear methods}{13}{part.1}
\contentsline {chapter}{\numberline {1}Quasi maximum likelihood estimation in linear models}{17}{chapter.1}
\contentsline {section}{\numberline {1.1}Linear Modeling}{17}{section.1.1}
\contentsline {subsection}{\numberline {1.1.1}Estimation under homogeneous noise assumption}{19}{subsection.1.1.1}
\contentsline {subsection}{\numberline {1.1.2}Linear basis transformation}{20}{subsection.1.1.2}
\contentsline {subsection}{\numberline {1.1.3}Orthogonal and orthonormal design}{22}{subsection.1.1.3}
\contentsline {subsection}{\numberline {1.1.4}Spectral representation}{23}{subsection.1.1.4}
\contentsline {section}{\numberline {1.2}Properties of the response estimate $\tmspace +\thinmuskip {.1667em} \setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \boldsymbol {f}$}\mathaccent "0365{\boldsymbol {f}} \tmspace +\thinmuskip {.1667em}$}{24}{section.1.2}
\contentsline {subsection}{\numberline {1.2.1}Decomposition into a deterministic and a stochastic component}{24}{subsection.1.2.1}
\contentsline {subsection}{\numberline {1.2.2}Properties of the operator $\tmspace +\thinmuskip {.1667em} \varPi \tmspace +\thinmuskip {.1667em}$}{25}{subsection.1.2.2}
\contentsline {subsection}{\numberline {1.2.3}Quadratic loss and risk of the response estimation}{26}{subsection.1.2.3}
\contentsline {subsection}{\numberline {1.2.4}Misspecified ``colored noise''}{27}{subsection.1.2.4}
\contentsline {section}{\numberline {1.3}Properties of the MLE $\tmspace +\thinmuskip {.1667em} \setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \boldsymbol {\theta }$}\mathaccent "0365{\boldsymbol {\theta }} \tmspace +\thinmuskip {.1667em}$}{28}{section.1.3}
\contentsline {subsection}{\numberline {1.3.1}Properties of the stochastic component}{28}{subsection.1.3.1}
\contentsline {subsection}{\numberline {1.3.2}Properties of the deterministic component}{29}{subsection.1.3.2}
\contentsline {subsection}{\numberline {1.3.3}Risk of estimation. R-efficiency}{31}{subsection.1.3.3}
\contentsline {subsubsection}{A colored noise}{33}{section*.2}
\contentsline {subsubsection}{A misspecified LPA}{33}{section*.3}
\contentsline {subsection}{\numberline {1.3.4}The case of a misspecified noise}{33}{subsection.1.3.4}
\contentsline {section}{\numberline {1.4}Linear models and quadratic log-likelihood}{34}{section.1.4}
\contentsline {subsection}{\numberline {1.4.1}Inference based on the maximum likelihood}{37}{subsection.1.4.1}
\contentsline {subsection}{\numberline {1.4.2}A misspecified LPA}{40}{subsection.1.4.2}
\contentsline {subsection}{\numberline {1.4.3}A misspecified noise structure}{40}{subsection.1.4.3}
\contentsline {chapter}{\numberline {2}Linear regression with random design}{43}{chapter.2}
\contentsline {section}{\numberline {2.1}Random design linear regression}{43}{section.2.1}
\contentsline {section}{\numberline {2.2}Design matrix and design distribution}{43}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Design with independent measurements}{44}{subsection.2.2.1}
\contentsline {subsection}{\numberline {2.2.2}Aggregated random design}{45}{subsection.2.2.2}
\contentsline {section}{\numberline {2.3}Fisher and Wilks expansions for the MLE under random design}{47}{section.2.3}
\contentsline {section}{\numberline {2.4}A deviation bound for $\tmspace +\thinmuskip {.1667em} \boldsymbol {\xi }\tmspace +\thinmuskip {.1667em}$}{50}{section.2.4}
\contentsline {section}{\numberline {2.5}Misspecified linear modeling assumption}{51}{section.2.5}
\contentsline {section}{\numberline {2.6}Application to instrumental regression}{54}{section.2.6}
\contentsline {chapter}{\numberline {3}Linear smoothers}{57}{chapter.3}
\contentsline {section}{\numberline {3.1}Regularization and ridge regression}{58}{section.3.1}
\contentsline {section}{\numberline {3.2}Penalized likelihood. Bias and variance}{58}{section.3.2}
\contentsline {section}{\numberline {3.3}Inference for the penalized MLE}{61}{section.3.3}
\contentsline {section}{\numberline {3.4}Projection and shrinkage estimates}{62}{section.3.4}
\contentsline {section}{\numberline {3.5}Smoothness constraints and roughness penalty approach}{64}{section.3.5}
\contentsline {section}{\numberline {3.6}Shrinkage in a linear inverse problem}{65}{section.3.6}
\contentsline {section}{\numberline {3.7}Spectral cut-off and spectral penalization. Diagonal estimates}{65}{section.3.7}
\contentsline {section}{\numberline {3.8}Roughness penalty and random design}{67}{section.3.8}
\contentsline {chapter}{\numberline {4}Sieve model selection in linear models}{73}{chapter.4}
\contentsline {section}{\numberline {4.1}Projection estimation. Loss and risk}{73}{section.4.1}
\contentsline {subsection}{\numberline {4.1.1}A linear model}{73}{subsection.4.1.1}
\contentsline {subsection}{\numberline {4.1.2}Linear decomposition of the estimator $\tmspace +\thinmuskip {.1667em} \setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \boldsymbol {\theta }$}\mathaccent "0365{\boldsymbol {\theta }} \tmspace +\thinmuskip {.1667em}$ and quadratic risk}{74}{subsection.4.1.2}
\contentsline {subsection}{\numberline {4.1.3}The case of Inhomogeneous errors}{74}{subsection.4.1.3}
\contentsline {subsection}{\numberline {4.1.4}Prediction error $\tmspace +\thinmuskip {.1667em} \setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \boldsymbol {f}$}\mathaccent "0365{\boldsymbol {f}} - \boldsymbol {f}^{*}\tmspace +\thinmuskip {.1667em}$}{75}{subsection.4.1.4}
\contentsline {subsection}{\numberline {4.1.5}Quadratic loss. Bias-variance decomposition}{75}{subsection.4.1.5}
\contentsline {subsection}{\numberline {4.1.6}Projection estimation and the model choice problem}{76}{subsection.4.1.6}
\contentsline {section}{\numberline {4.2}Unbiased risk estimation}{78}{section.4.2}
\contentsline {subsection}{\numberline {4.2.1}AIC and pairwise comparison}{80}{subsection.4.2.1}
\contentsline {subsection}{\numberline {4.2.2}Pairwise analysis}{82}{subsection.4.2.2}
\contentsline {subsection}{\numberline {4.2.3}Uniform bounds and the zone of insensitivity}{84}{subsection.4.2.3}
\contentsline {subsection}{\numberline {4.2.4}A bound on the excess}{85}{subsection.4.2.4}
\contentsline {section}{\numberline {4.3}The approach based on multiple testing. ``Smallest accepted'' rule}{87}{section.4.3}
\contentsline {subsection}{\numberline {4.3.1}A LR test}{88}{subsection.4.3.1}
\contentsline {subsection}{\numberline {4.3.2}Multiplicity correction}{89}{subsection.4.3.2}
\contentsline {subsection}{\numberline {4.3.3}Definition of the oracle and propagation property}{91}{subsection.4.3.3}
\contentsline {subsection}{\numberline {4.3.4}A bound on the loss}{92}{subsection.4.3.4}
\contentsline {subsection}{\numberline {4.3.5}Role of $\tmspace +\thinmuskip {.1667em} \beta \tmspace +\thinmuskip {.1667em}$}{94}{subsection.4.3.5}
\contentsline {chapter}{\numberline {5}Ordered model selection for linear smoothers}{95}{chapter.5}
\contentsline {section}{\numberline {5.1}Introduction}{95}{section.5.1}
\contentsline {section}{\numberline {5.2}SmA procedure. Known noise}{98}{section.5.2}
\contentsline {subsection}{\numberline {5.2.1}Model and problem}{98}{subsection.5.2.1}
\contentsline {paragraph}{Estimation of the whole vector $\tmspace +\thinmuskip {.1667em} \boldsymbol {\theta }^{*}\tmspace +\thinmuskip {.1667em}$}{99}{section*.4}
\contentsline {paragraph}{Prediction}{99}{section*.5}
\contentsline {paragraph}{Semiparametric estimation}{99}{section*.6}
\contentsline {paragraph}{Linear functional estimation}{99}{section*.7}
\contentsline {subsection}{\numberline {5.2.2}Smallest accepted (SmA) method in ordered model selection}{101}{subsection.5.2.2}
\contentsline {subsection}{\numberline {5.2.3}A ``good'' model}{102}{subsection.5.2.3}
\contentsline {subsection}{\numberline {5.2.4}Tail function, multiplicity correction, FWER, and critical values $\tmspace +\thinmuskip {.1667em} \mathfrak {z}_{m,m^{\circ }} \tmspace +\thinmuskip {.1667em}$}{103}{subsection.5.2.4}
\contentsline {subsection}{\numberline {5.2.5}SmA procedure and propagation property for known noise}{105}{subsection.5.2.5}
\contentsline {section}{\numberline {5.3}Bootstrap tuning}{106}{section.5.3}
\contentsline {subsection}{\numberline {5.3.1}Presmoothing and wild boostrap}{106}{subsection.5.3.1}
\contentsline {section}{\numberline {5.4}Theoretical properties}{108}{section.5.4}
\contentsline {subsection}{\numberline {5.4.1}Known noise}{108}{subsection.5.4.1}
\contentsline {subsection}{\numberline {5.4.2}Power loss function}{110}{subsection.5.4.2}
\contentsline {subsection}{\numberline {5.4.3}Analysis of the payment for adaptation $\tmspace +\thinmuskip {.1667em} \overline {\mathfrak {z}}_{m^{*}} \tmspace +\thinmuskip {.1667em}$}{113}{subsection.5.4.3}
\contentsline {subsection}{\numberline {5.4.4}Application to projection estimation}{114}{subsection.5.4.4}
\contentsline {subsection}{\numberline {5.4.5}Linear functional estimation}{115}{subsection.5.4.5}
\contentsline {subsection}{\numberline {5.4.6}Validity of the bootstrap procedure. Conditions}{117}{subsection.5.4.6}
\contentsline {subsection}{\numberline {5.4.7}Bootstrap validation. Range of applicability}{118}{subsection.5.4.7}
\contentsline {section}{\numberline {5.5}Bootstrap validity and critical dimension}{119}{section.5.5}
\contentsline {section}{\numberline {5.6}Simulations}{120}{section.5.6}
\contentsline {section}{\numberline {5.7}Proofs}{124}{section.5.7}
\contentsline {subsection}{\numberline {5.7.1}Proof of Theorems~\ref {ToracleSmA} and \ref {ToracleSmAes}}{124}{subsection.5.7.1}
\contentsline {subsection}{\numberline {5.7.2}Proof of Theorem~\ref {Tpayment}}{125}{subsection.5.7.2}
\contentsline {subsection}{\numberline {5.7.3}Proof of Theorem~\ref {TSmApoly}}{126}{subsection.5.7.3}
\contentsline {subsection}{\numberline {5.7.4}Proof of Proposition~\ref {Talpmxxm}}{127}{subsection.5.7.4}
\contentsline {subsection}{\numberline {5.7.5}Proof of Theorem~\ref {TGaussbootB}}{128}{subsection.5.7.5}
\contentsline {subsection}{\numberline {5.7.6}Proof of Theorem~\ref {TGaussbootB2}}{129}{subsection.5.7.6}
\contentsline {subsection}{\numberline {5.7.7}Proof of Theorem~\ref {Tbootmain}}{130}{subsection.5.7.7}
\contentsline {section}{\numberline {5.8}Linear non-Gaussian case and GAR}{130}{section.5.8}
\contentsline {chapter}{\numberline {6}Unordered case. Anisotropic sets and subset selection}{133}{chapter.6}
\contentsline {section}{\numberline {6.1}Subset selection procedure}{133}{section.6.1}
\contentsline {subsection}{\numberline {6.1.1}SmA procedure and multilevel synchronization}{134}{subsection.6.1.1}
\contentsline {subsection}{\numberline {6.1.2}Prediction loss}{137}{subsection.6.1.2}
\contentsline {subsection}{\numberline {6.1.3}Estimation loss}{138}{subsection.6.1.3}
\contentsline {subsection}{\numberline {6.1.4}Linear functional estimation}{139}{subsection.6.1.4}
\contentsline {subsection}{\numberline {6.1.5}Subset selection problem}{140}{subsection.6.1.5}
\contentsline {section}{\numberline {6.2}Anisotropic models}{141}{section.6.2}
\contentsline {chapter}{\numberline {7}SmA and parameter tuning in high dimensional regression}{145}{chapter.7}
\contentsline {section}{\numberline {7.1}SmA subset selection in high dimensional regression}{146}{section.7.1}
\contentsline {chapter}{\numberline {8}Penalized model selection}{151}{chapter.8}
\contentsline {section}{\numberline {8.1}Complexity penalization}{151}{section.8.1}
\contentsline {subsection}{\numberline {8.1.1}Orthonormal case}{152}{subsection.8.1.1}
\contentsline {subsection}{\numberline {8.1.2}Penalty tuning using propagation condition}{153}{subsection.8.1.2}
\contentsline {subsection}{\numberline {8.1.3}Oracle inequality for $\tmspace +\thinmuskip {.1667em} \setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \varkappa $}\mathaccent "0362{\varkappa } \tmspace +\thinmuskip {.1667em}$-choice}{155}{subsection.8.1.3}
\contentsline {subsection}{\numberline {8.1.4}Bootstrap based tuning of penalty}{156}{subsection.8.1.4}
\contentsline {section}{\numberline {8.2}Sparse penalty}{156}{section.8.2}
\contentsline {subsection}{\numberline {8.2.1}Basic inequality}{158}{subsection.8.2.1}
\contentsline {subsection}{\numberline {8.2.2}Dual problem and Danzig selector}{160}{subsection.8.2.2}
\contentsline {subsection}{\numberline {8.2.3}Data-driven choice of $\tmspace +\thinmuskip {.1667em} \lambda \tmspace +\thinmuskip {.1667em}$}{160}{subsection.8.2.3}
\contentsline {part}{Part\ II\hskip \betweenumberspace General parametric theory}{161}{part.2}
\contentsline {chapter}{\numberline {9}Fisher and Wilks expansion}{165}{chapter.9}
\contentsline {section}{\numberline {9.1}Main results}{166}{section.9.1}
\contentsline {section}{\numberline {9.2}Non-Gaussian case: conditions}{167}{section.9.2}
\contentsline {section}{\numberline {9.3}Properties of the MLE $\tmspace +\thinmuskip {.1667em} \setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \boldsymbol {\theta }$}\mathaccent "0365{\boldsymbol {\theta }} \tmspace +\thinmuskip {.1667em}$}{170}{section.9.3}
\contentsline {section}{\numberline {9.4}Some auxiliary results and proofs}{172}{section.9.4}
\contentsline {subsection}{\numberline {9.4.1}Local linear approximation of the gradient of the log-likelihood}{172}{subsection.9.4.1}
\contentsline {subsection}{\numberline {9.4.2}Local quadratic approximation of the log-likelihood}{174}{subsection.9.4.2}
\contentsline {subsection}{\numberline {9.4.3}Proof of Theorem~\ref {TMLE}}{175}{subsection.9.4.3}
\contentsline {subsection}{\numberline {9.4.4}Proof of Theorem~\ref {Tconflocro}}{176}{subsection.9.4.4}
\contentsline {subsection}{\numberline {9.4.5}Proof of Theorem~\ref {TWilks2r}}{176}{subsection.9.4.5}
\contentsline {chapter}{\numberline {10}Bernstein -- von Mises Theorem}{179}{chapter.10}
\contentsline {section}{\numberline {10.1}Parametric BvM Theorem}{180}{section.10.1}
\contentsline {section}{\numberline {10.2}The use of posterior mean and variance for credible sets}{181}{section.10.2}
\contentsline {section}{\numberline {10.3}Extension to a flat Gaussian prior}{183}{section.10.3}
\contentsline {section}{\numberline {10.4}Proof of Theorem~\ref {TBvM}}{184}{section.10.4}
\contentsline {subsection}{\numberline {10.4.1}Local Gaussian approximation of the posterior. Upper bound}{185}{subsection.10.4.1}
\contentsline {subsection}{\numberline {10.4.2}Tail posterior probability and contraction}{187}{subsection.10.4.2}
\contentsline {subsection}{\numberline {10.4.3}Local Gaussian approximation of the posterior. Lower bound}{189}{subsection.10.4.3}
\contentsline {subsection}{\numberline {10.4.4}Moments of the posterior}{190}{subsection.10.4.4}
\contentsline {section}{\numberline {10.5}Proof of Theorem~\ref {TGaussprior}}{191}{section.10.5}
\contentsline {chapter}{\numberline {11}Roughness penalty for dimension reduction}{193}{chapter.11}
\contentsline {section}{\numberline {11.1}Fisher and Wilks Theorems under quadratic penalization}{196}{section.11.1}
\contentsline {section}{\numberline {11.2}Effective dimension}{198}{section.11.2}
\contentsline {paragraph}{Block penalization}{198}{section*.8}
\contentsline {paragraph}{Sobolev smoothness constraint}{199}{section*.9}
\contentsline {paragraph}{Linear inverse problem}{199}{section*.10}
\contentsline {section}{\numberline {11.3}Conditions}{199}{section.11.3}
\contentsline {section}{\numberline {11.4}Concentration and a large deviation bound}{202}{section.11.4}
\contentsline {section}{\numberline {11.5}Wilks and Fisher expansions}{204}{section.11.5}
\contentsline {section}{\numberline {11.6}Quadratic risk bound and modeling bias}{205}{section.11.6}
\contentsline {section}{\numberline {11.7}Proofs of the Fisher and Wilks expansions}{207}{section.11.7}
\contentsline {section}{\numberline {11.8}Nonparametric BvM Theorem: Gaussian case}{210}{section.11.8}
\contentsline {subsection}{\numberline {11.8.1}Finite dimensional projections and maxispaces}{212}{subsection.11.8.1}
\contentsline {subsection}{\numberline {11.8.2}Concentration sets for the posterior}{214}{subsection.11.8.2}
\contentsline {subsection}{\numberline {11.8.3}Frequentist coverage for Bayesian credible sets}{215}{subsection.11.8.3}
\contentsline {subsection}{\numberline {11.8.4}Non-Gaussian errors}{217}{subsection.11.8.4}
\contentsline {section}{\numberline {11.9}Nonparametric BvM Theorem: non-Gaussian case}{218}{section.11.9}
\contentsline {subsection}{\numberline {11.9.1}A linear stochastic term}{219}{subsection.11.9.1}
\contentsline {subsection}{\numberline {11.9.2}General likelihood}{225}{subsection.11.9.2}
\contentsline {chapter}{\numberline {12}Semiparametric estimation}{229}{chapter.12}
\contentsline {section}{\numberline {12.1}Fisher and Wilks results for a parameter subvector}{230}{section.12.1}
\contentsline {subsection}{\numberline {12.1.1}Fisher expansion and semiparametric concentration}{231}{subsection.12.1.1}
\contentsline {subsection}{\numberline {12.1.2}Semiparametric Wilks expansion}{233}{subsection.12.1.2}
\contentsline {section}{\numberline {12.2}Likelihood ratio test statistic for a composite hypothesis}{234}{section.12.2}
\contentsline {section}{\numberline {12.3}Semiparametric BvM approximation}{236}{section.12.3}
\contentsline {section}{\numberline {12.4}Sieve semiparametric inference}{241}{section.12.4}
\contentsline {section}{\numberline {12.5}Estimation of a nonlinear functional}{241}{section.12.5}
\contentsline {section}{\numberline {12.6}Bias in semiparametric sieve approximation}{243}{section.12.6}
\contentsline {subsection}{\numberline {12.6.1}Basis transformation for the nuisance}{244}{subsection.12.6.1}
\contentsline {subsection}{\numberline {12.6.2}Smoothness conditions}{245}{subsection.12.6.2}
\contentsline {chapter}{\numberline {13}Parametric i.i.d. models}{251}{chapter.13}
\contentsline {section}{\numberline {13.1}Quasi MLE in an i.i.d. model}{251}{section.13.1}
\contentsline {section}{\numberline {13.2}Conditions in the i.i.d. case}{252}{section.13.2}
\contentsline {section}{\numberline {13.3}Results in the non-penalized i.i.d. case}{254}{section.13.3}
\contentsline {section}{\numberline {13.4}Roughness penalization for an i.i.d. sample}{255}{section.13.4}
\contentsline {section}{\numberline {13.5}BvM Theorem for the i.i.d. data}{257}{section.13.5}
\contentsline {chapter}{\numberline {14}Generalized linear models}{259}{chapter.14}
\contentsline {section}{\numberline {14.1}Linear models}{259}{section.14.1}
\contentsline {section}{\numberline {14.2}Generalized linear models (GLM)}{261}{section.14.2}
\contentsline {subsection}{\numberline {14.2.1}A general deviation bound for the MLE $\tmspace +\thinmuskip {.1667em} \setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \boldsymbol {\theta }$}\mathaccent "0365{\boldsymbol {\theta }} \tmspace +\thinmuskip {.1667em}$}{262}{subsection.14.2.1}
\contentsline {subsection}{\numberline {14.2.2}Fisher and Wilks expansions for $\tmspace +\thinmuskip {.1667em} \setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \boldsymbol {\theta }$}\mathaccent "0365{\boldsymbol {\theta }} \tmspace +\thinmuskip {.1667em}$}{263}{subsection.14.2.2}
\contentsline {subsection}{\numberline {14.2.3}Sufficient conditions on design and errors}{264}{subsection.14.2.3}
\contentsline {section}{\numberline {14.3}Nonparametric sieve GLM estimation}{268}{section.14.3}
\contentsline {section}{\numberline {14.4}Estimation for a penalized GLM}{270}{section.14.4}
\contentsline {section}{\numberline {14.5}BvM Theorem for a GLM}{272}{section.14.5}
\contentsline {subsection}{\numberline {14.5.1}A non-informative prior}{272}{subsection.14.5.1}
\contentsline {subsection}{\numberline {14.5.2}Nonparametric BvM with a Gaussian prior}{275}{subsection.14.5.2}
\contentsline {section}{\numberline {14.6}GLM with random design}{276}{section.14.6}
\contentsline {subsection}{\numberline {14.6.1}Local concentration of $\tmspace +\thinmuskip {.1667em} \setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \boldsymbol {\theta }$}\mathaccent "0365{\boldsymbol {\theta }} \tmspace +\thinmuskip {.1667em}$}{278}{subsection.14.6.1}
\contentsline {subsection}{\numberline {14.6.2}Fisher and Wilks expansions}{279}{subsection.14.6.2}
\contentsline {subsection}{\numberline {14.6.3}Sufficient conditions for the case of random design}{280}{subsection.14.6.3}
\contentsline {subsection}{\numberline {14.6.4}Nonparametric BvM for a Gaussian prior}{285}{subsection.14.6.4}
\contentsline {chapter}{\numberline {15}Estimation of a log-density}{287}{chapter.15}
\contentsline {section}{\numberline {15.1}Log-density estimation. Conditions}{287}{section.15.1}
\contentsline {section}{\numberline {15.2}Sieve nonparametric density estimation}{294}{section.15.2}
\contentsline {section}{\numberline {15.3}Sieve likelihood ratio test}{300}{section.15.3}
\contentsline {section}{\numberline {15.4}Error of estimation for the log-density function}{301}{section.15.4}
\contentsline {subsection}{\numberline {15.4.1}Kullback-Leibler divergence}{302}{subsection.15.4.1}
\contentsline {subsection}{\numberline {15.4.2}Hellinger loss}{303}{subsection.15.4.2}
\contentsline {section}{\numberline {15.5}Penalized smooth density estimation}{305}{section.15.5}
\contentsline {subsection}{\numberline {15.5.1}Loss in penalized density estimation}{309}{subsection.15.5.1}
\contentsline {section}{\numberline {15.6}Parametric structural log-density modeling}{310}{section.15.6}
\contentsline {chapter}{\numberline {16}Sieve parametric approach in nonparametric regression}{317}{chapter.16}
\contentsline {section}{\numberline {16.1}Parametric and nonparametric regression}{317}{section.16.1}
\contentsline {section}{\numberline {16.2}Conditions}{318}{section.16.2}
\contentsline {subsection}{\numberline {16.2.1}Checking the local conditions \nameref {ED0ref} and \nameref {ED2ref}}{320}{subsection.16.2.1}
\contentsline {subsection}{\numberline {16.2.2}Checking the local condition \nameref {LL0ref}}{321}{subsection.16.2.2}
\contentsline {section}{\numberline {16.3}Large deviation result and Fisher expansion}{326}{section.16.3}
\contentsline {section}{\numberline {16.4}Prediction loss and bias-variance decomposition}{327}{section.16.4}
\contentsline {section}{\numberline {16.5}Sieve nonparametric estimation}{328}{section.16.5}
\contentsline {section}{\numberline {16.6}Penalized regression}{329}{section.16.6}
\contentsline {section}{\numberline {16.7}Semiparametric problem}{330}{section.16.7}
\contentsline {section}{\numberline {16.8}Random design regression}{334}{section.16.8}
\contentsline {subsection}{\numberline {16.8.1}Checking the condition \nameref {LL0ref}}{336}{subsection.16.8.1}
\contentsline {subsection}{\numberline {16.8.2}Checking the conditions \nameref {ED0ref} and \nameref {ED2ref}}{337}{subsection.16.8.2}
\contentsline {chapter}{\numberline {17}Structural regression}{343}{chapter.17}
\contentsline {section}{\numberline {17.1}Single-index case}{343}{section.17.1}
\contentsline {subsubsection}{Identifiability issue for the index vector}{347}{section*.11}
\contentsline {subsubsection}{Sobolev smoothness of the link function and the use of cosine basis}{347}{section*.12}
\contentsline {subsubsection}{Index-vector estimation under smoothness of $\tmspace +\thinmuskip {.1667em} g\tmspace +\thinmuskip {.1667em}$}{348}{section*.13}
\contentsline {section}{\numberline {17.2}Error-in-variable nonparametric regression}{349}{section.17.2}
\contentsline {subsubsection}{Cosine-basis and sieve approximation}{352}{section*.14}
\contentsline {subsubsection}{Lower bound}{353}{section*.15}
\contentsline {subsubsection}{Impact of the design variance $\tmspace +\thinmuskip {.1667em} \sigma ^{2} \tmspace +\thinmuskip {.1667em}$}{353}{section*.16}
\contentsline {section}{\numberline {17.3}Instrumental regression}{353}{section.17.3}
\contentsline {subsubsection}{Plug-in method}{354}{section*.17}
\contentsline {subsubsection}{Semiparametric approach}{355}{section*.18}
\contentsline {subsubsection}{Required conditions}{357}{section*.19}
\contentsline {subsubsection}{Main results}{357}{section*.20}
\contentsline {chapter}{\numberline {18}Median and quantile regression}{359}{chapter.18}
\contentsline {chapter}{\numberline {19}Generalized regression}{361}{chapter.19}
\contentsline {part}{Part\ III\hskip \betweenumberspace Structural regression}{363}{part.3}
\contentsline {chapter}{\numberline {20}Sieve Model Selection}{365}{chapter.20}
\contentsline {section}{\numberline {20.1}Sieve SmA procedure}{365}{section.20.1}
\contentsline {section}{\numberline {20.2}Resampling methods for parameter tuning in generalized regression}{367}{section.20.2}
\contentsline {subsection}{\numberline {20.2.1}Generalized regression}{367}{subsection.20.2.1}
\contentsline {subsection}{\numberline {20.2.2}Multiplier bootstrap}{368}{subsection.20.2.2}
\contentsline {subsection}{\numberline {20.2.3}Numerical issues}{370}{subsection.20.2.3}
\contentsline {section}{\numberline {20.3}Sieve Generalized Linear regression}{371}{section.20.3}
\contentsline {subsection}{\numberline {20.3.1}Sieve MLE}{372}{subsection.20.3.1}
\contentsline {subsection}{\numberline {20.3.2}Bootstrap counterpart}{373}{subsection.20.3.2}
\contentsline {subsection}{\numberline {20.3.3}Bootstrap for the SmA procedure}{374}{subsection.20.3.3}
\contentsline {part}{Part\ IV\hskip \betweenumberspace Mathematical tools}{375}{part.4}
\contentsline {chapter}{\numberline {A}Some results for Gaussian law}{377}{appendix.A}
\contentsline {section}{\numberline {A.1}Deviation bounds for a Gaussian vector}{377}{section.A.1}
\contentsline {section}{\numberline {A.2}Gaussian integrals}{378}{section.A.2}
\contentsline {chapter}{\numberline {B}Deviation bounds for quadratic forms}{383}{appendix.B}
\contentsline {section}{\numberline {B.1}Gaussian quadratic forms}{383}{section.B.1}
\contentsline {section}{\numberline {B.2}Deviation bounds for non-Gaussian quadratic forms}{386}{section.B.2}
\contentsline {subsection}{\numberline {B.2.1}Deviation bounds for the norm of a standardized non-Gaussian vector}{387}{subsection.B.2.1}
\contentsline {subsection}{\numberline {B.2.2}A deviation bound for a general non-Gaussian quadratic form }{391}{subsection.B.2.2}
\contentsline {section}{\numberline {B.3}Deviation probability for a normalized martingale}{394}{section.B.3}
\contentsline {chapter}{\numberline {C}Sums of random matrices}{397}{appendix.C}
\contentsline {section}{\numberline {C.1}Matrix Bernstein inequality}{397}{section.C.1}
\contentsline {section}{\numberline {C.2}Presmoothing and bias effects}{404}{section.C.2}
\contentsline {paragraph}{Bounds for $\tmspace +\thinmuskip {.1667em} \EuScript {B}_{1} \tmspace +\thinmuskip {.1667em}$}{405}{section*.21}
\contentsline {paragraph}{Bounds for $\tmspace +\thinmuskip {.1667em} \EuScript {B}_{2} \tmspace +\thinmuskip {.1667em}$}{406}{section*.22}
\contentsline {paragraph}{Bounds for $\tmspace +\thinmuskip {.1667em} \EuScript {B}_{3} \tmspace +\thinmuskip {.1667em}$}{406}{section*.23}
\contentsline {paragraph}{Bounds for $\tmspace +\thinmuskip {.1667em} \EuScript {B}_{4} \tmspace +\thinmuskip {.1667em}$}{406}{section*.24}
\contentsline {paragraph}{Bounds for $\tmspace +\thinmuskip {.1667em} \EuScript {B}_{5} \tmspace +\thinmuskip {.1667em}$}{406}{section*.25}
\contentsline {section}{\numberline {C.3}Empirical covariance matrix}{407}{section.C.3}
\contentsline {chapter}{\numberline {D}Gaussian comparison via KL-divergence and Pinsker's inequality}{409}{appendix.D}
\contentsline {section}{\numberline {D.1}Pinsker's inequality}{409}{section.D.1}
\contentsline {section}{\numberline {D.2}Gaussian comparison}{410}{section.D.2}
\contentsline {chapter}{\numberline {E}Random multiplicity correction}{413}{appendix.E}
\contentsline {section}{\numberline {E.1}Gaussian measures with random covariance}{413}{section.E.1}
\contentsline {section}{\numberline {E.2}Max-case}{415}{section.E.2}
\contentsline {chapter}{\numberline {F}High-dimensional inference for a Gaussian law}{417}{appendix.F}
\contentsline {section}{\numberline {F.1}Stein identity, Slepian bridge, and Gaussian comparison}{417}{section.F.1}
\contentsline {section}{\numberline {F.2}Comparing of the maximum of Gaussians}{420}{section.F.2}
\contentsline {section}{\numberline {F.3}Anti-concentration for Gaussian maxima}{421}{section.F.3}
\contentsline {section}{\numberline {F.4}Gaussian comparison for the squared norm}{422}{section.F.4}
\contentsline {section}{\numberline {F.5}Approximation of the indicator function}{424}{section.F.5}
\contentsline {chapter}{\numberline {G}Gaussian approximation of a vector sum}{427}{appendix.G}
\contentsline {section}{\numberline {G.1}A univariate case with Lindeberg telescopic sums}{427}{section.G.1}
\contentsline {section}{\numberline {G.2}Berry-Esseen Theorem for a univariate sum}{429}{section.G.2}
\contentsline {subsection}{\numberline {G.2.1}Characteristic functions for a univariate sum}{430}{subsection.G.2.1}
\contentsline {subsection}{\numberline {G.2.2}Characteristic function of a sum. Simmerization}{432}{subsection.G.2.2}
\contentsline {subsection}{\numberline {G.2.3}Methods based on the Fourier-Stieltjes transform}{435}{subsection.G.2.3}
\contentsline {subsection}{\numberline {G.2.4}Berry-Esseen Theorem}{438}{subsection.G.2.4}
\contentsline {subsection}{\numberline {G.2.5}Fourier transform for the norm of a vector}{439}{subsection.G.2.5}
\contentsline {subsection}{\numberline {G.2.6}Fourier transform for the squared norm of a vector}{442}{subsection.G.2.6}
\contentsline {section}{\numberline {G.3}GAR for the Euclidean norm of a vector sum}{443}{section.G.3}
\contentsline {section}{\numberline {G.4}GAR for the sup-norm of a vector sum}{447}{section.G.4}
\contentsline {section}{\numberline {G.5}GAR for the sup-norm of a vector sum. Improved}{450}{section.G.5}
\contentsline {section}{\numberline {G.6}GAR for weighted sums}{453}{section.G.6}
\contentsline {section}{\numberline {G.7}A uniform bound for the maximum of the norm of weighted vector sums}{454}{section.G.7}
\contentsline {chapter}{\numberline {H}Deviation bounds for random processes}{457}{appendix.H}
\contentsline {section}{\numberline {H.1} Chaining and covering numbers}{457}{section.H.1}
\contentsline {section}{\numberline {H.2} Entropy and Dudley's integral}{463}{section.H.2}
\contentsline {section}{\numberline {H.3} A local bound with generic chaining}{463}{section.H.3}
\contentsline {section}{\numberline {H.4} Generic chaining with partitioning}{466}{section.H.4}
\contentsline {section}{\numberline {H.5} A large deviation bound}{466}{section.H.5}
\contentsline {section}{\numberline {H.6} Finite-dimensional smooth case}{467}{section.H.6}
\contentsline {subsection}{\numberline {H.6.1}Covering and entropy for Euclidean distance}{468}{subsection.H.6.1}
\contentsline {subsection}{\numberline {H.6.2}Generic chaining}{470}{subsection.H.6.2}
\contentsline {section}{\numberline {H.7} Entropy of an ellipsoid}{471}{section.H.7}
\contentsline {section}{\numberline {H.8} Roughness constraints for dimension reduction}{476}{section.H.8}
\contentsline {section}{\numberline {H.9} Bound for a bivariate process}{477}{section.H.9}
\contentsline {section}{\numberline {H.10} A bound for the norm of a vector random process}{479}{section.H.10}
\contentsline {section}{\numberline {H.11} A bound for a family of quadratic forms}{480}{section.H.11}
\contentsline {section}{\numberline {H.12} A bound for a smooth quadratic field}{481}{section.H.12}
\contentsline {chapter}{References}{483}{appendix*.26}
